# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#L32
seed: 42

# epoch * n_steps * nenvs: 4500*250*1024
# n_timesteps: 16384000
n_timesteps: 1152000000
policy: 'MlpPolicy'
device: 'cuda:0'
n_steps: 512
# mini batch size: num_envs * nsteps / nminibatches 2048ร512รท2048
batch_size: 192
gae_lambda: 0.95
gamma: 0.99
n_epochs: 10
ent_coef: 0.00
vf_coef: 0.0001
learning_rate: !!float 1e-4
clip_range: 0.2
normalize_advantage: True
policy_kwargs: "dict(
                  activation_fn=nn.ELU,
                  net_arch=[dict(pi=[256, 128, 64], vf=[256, 128, 64])]
                )"
target_kl: 0.01
max_grad_norm: 1.0

# # Uses VecNormalize class to normalize obs
# normalize_input: True
# # Uses VecNormalize class to normalize rew
# normalize_value: True
# clip_obs: 5
